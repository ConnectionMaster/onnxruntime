#pragma once
#include "core/framework/op_kernel.h"
#include "core/framework/func_api.h"
#include "core/graph/function.h"
namespace onnxruntime {

void* allocate_helper_func(void* allocator, size_t alignment, size_t size);

void release_helper_func(void* allocator, void* p);

DType ORT_type_to_c_type(MLDataType type);

//A kernel that wrapper the ComputeFunction call generated by execution provider when fuse the sub-graph
class FunctionKernel : public OpKernel {
 public:
  //The original design is we load the dll, find the entry point and wrapper it.
  //Here for quick prototype, we keep the entry pointer in the node.
  explicit FunctionKernel(const OpKernelInfo& info) : OpKernel(info) {
    num_inputs_ = info.node().InputDefs().size();
    num_outputs_ = info.node().OutputDefs().size();
    CreateFunctionStateFunc create_func;
    auto status = info.GetFusedFuncs(&func_, &create_func, &release_func_);
    ORT_ENFORCE(status.IsOK(), status.ErrorMessage());
    if (create_func) {
      //TODO: we are only provide host allocate method in compute context.
      std::set<AllocatorPtr> output_allocators;
      for (size_t i = 0; i < info.GetOutputCount(); ++i) {
        AllocatorPtr alloc;
        //If get output tensor's allocator failed, means the output is a non-tensor value, or its memory is reused by other tensors.
        //We don't support it now.
        ORT_ENFORCE(info.GetOutputTensorAllocator(i, alloc).IsOK());
        //if i-th output is optional, the alloc will be nullptr. ignore it.
        if (alloc)
          output_allocators.insert(alloc);
      }
      // currently in function api, we only pass in 1 allocator. need further extension if the outputs are located at different allocators.
      ORT_ENFORCE(output_allocators.size() == 1);
      output_allocator_ = *output_allocators.begin();
      ComputeContext context = {allocate_helper_func, release_helper_func, output_allocator_.get(), info.node().Name().c_str()};
      ORT_ENFORCE(create_func(&context, &func_state_) == 0);
    }
  }

  virtual ~FunctionKernel() {
    if (release_func_ && func_state_) {
      release_func_(func_state_);
    }
  }

  virtual Status Compute(OpKernelContext* context) const override {
    std::vector<ONNXRunTimeTensor> input_tensors;
    for (int i = 0; i < num_inputs_; i++) {
      const Tensor* input = context->Input<Tensor>(i);
      auto& shape = input->Shape();
      auto& dims = shape.GetDims();
      ONNXRunTimeTensor input_tensor = {
          const_cast<void*>(input->DataRaw()),
          shape.NumDimensions(),
          //hard code to double now
          ORT_type_to_c_type(input->DataType()),
          dims.empty() ? nullptr : const_cast<int64_t*>(&dims[0])};
      input_tensors.push_back(input_tensor);
    }

    std::vector<ONNXRunTimeTensor> output_tensors(num_outputs_);
    int ret = func_(func_state_, input_tensors.empty() ? nullptr : &input_tensors[0], input_tensors.size(), &output_tensors[0], output_tensors.size());
    if (ret != 0)
      return Status(common::ONNXRUNTIME, common::FAIL, "FuncKernel call failed with error code: " + std::to_string(ret));

    for (int i = 0; i < num_outputs_; i++) {
      //since we don't want to re-allocate te output buffer, create a tensor with empty buffer in execution frame first.
      Tensor* output = context->Output(i, {0});
      TensorShape output_shape(std::vector<int64_t>(output_tensors[i].shape, output_tensors[i].shape + output_tensors[i].ndim));
      //swap the buffer created by compute method into the output tensor.
      ORT_RETURN_IF_ERROR(output->ReplaceBuffer(&output_tensors[i], output_allocator_));
      // for shape, becauset the output_allocator_ we use could be a device allocator, if the kernel is assigned to a device like gpu.
      // so we prefer to directly allocate shape on heap. otherwise we need pass in multile allocator function for host and device.
      delete[] output_tensors[i].shape;
    }

    return Status::OK();
  }

 private:
  ComputeFunc func_;
  DestroyFunctionStateFunc release_func_;
  FunctionState func_state_;
  size_t num_inputs_;
  size_t num_outputs_;
  AllocatorPtr output_allocator_;
};
}  // namespace onnxruntime
